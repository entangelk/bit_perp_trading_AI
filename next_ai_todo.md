# AI 시스템 최적화 작업 현황 및 할 일

## ✅ 완료된 작업

### 1. 투자 안전성 개선 (완료)
- **기본값 제거**: 모든 분석기에서 더미/기본값 완전 삭제
- **에러 카운팅 시스템**: 각 분석기별 API 실패 추적 (한도: 3회)
- **데이터 검증**: 신뢰할 수 없는 데이터로 투자 결정 방지
- **스킵 로직**: 데이터 부족 시 분석 건너뛰기 구현

### 2. 수정된 파일들
```
✅ sentiment_analyzer.py - 에러 카운팅 + 기본값 제거
✅ macro_analyzer.py - 에러 카운팅 + 기본값 제거
✅ onchain_analyzer.py - 에러 카운팅 + 기본값 제거
✅ institution_analyzer.py - 에러 카운팅 + 기본값 제거
✅ position_analyzer.py - 에러 카운팅 추가
✅ final_decisionmaker.py - 데이터 부족 시 결정 보류 로직
```

### 3. API 사용량 분석 (완료)
- **CoinGecko API**: 월 10,000회 한도로 충분 (11회/분석)
- **AI API 문제 발견**: 일 250회 한도에 672회 사용 (268% 초과)
- **비효율적 구조 발견**: 데이터 수집과 AI 분석이 분리되어 중복 작업

## 🚨 현재 문제점

### AI API 호출량 초과 (심각)
```
현재 구조:
├── 데이터 수집: 각자 다른 주기로 원시 데이터만 저장
└── 거래 결정: 15분마다 모든 분석기 AI 재실행 (7회 × 96 = 672회/일)

문제점:
├── 일 250회 한도 대비 268% 초과
├── 9시간 후 한도 소진
└── 같은 데이터로 중복 AI 호출
```

### 비효율적 아키텍처
- 공포/탐욕 지수: 4시간마다 변하는데 15분마다 AI 재분석
- 거시경제: 6시간마다 변하는데 15분마다 AI 재분석
- 완전히 2번 일하는 구조

## 🎯 수정해야 할 사항 (우선순위)

### 1. 긴급 수정 (AI API 한도 초과 해결)
```
📁 data_scheduler.py
└── 원시 데이터 수집 + AI 분석 결과까지 저장하도록 수정

📁 ai_trading_integration.py  
└── run_all_analyses()를 캐시된 분석 결과 조회로 변경

목표: 672회/일 → 200회/일 이하로 감소
```

### 2. API 호출 실패 처리 강화
```
📋 API 호출 실패 시 AI 분석 하지 않는 로직:
├── 데이터 수집 실패 → AI 분석 스킵
├── 에러 카운트 3회 이상 → 해당 분석기 비활성화
└── 최소 데이터 확보 실패 → 전체 분석 중단

📋 처리 방식:
├── 개별 분석기 실패: rule_based_analysis() 사용
├── 다수 분석기 실패: 최종 결정 보류
└── 전체 실패: 안전 모드 (거래 중단)
```

### 3. 최종 결정에 메타데이터 추가
```
📋 분석 시간 정보 포함:
├── 각 분석기별 데이터 수집 시간
├── AI 분석 완료 시간  
├── 데이터 신선도 (age)
└── 신뢰도 점수

📋 결정 품질 지표:
├── 사용된 분석기 개수
├── 데이터 품질 점수
├── 에러 발생 현황
└── 캐시 사용 비율
```

### 4. 아키텍처 최적화
```
📋 스마트 스케줄링:
├── sentiment: 30분마다 분석 + 저장
├── technical: 15분마다 분석 + 저장  
├── macro: 6시간마다 분석 + 저장
├── onchain: 1시간마다 분석 + 저장
├── institutional: 2시간마다 분석 + 저장
└── position + final: 실시간 (15분마다)

📋 조건부 AI 호출:
├── 변동성 높을 때: 모든 분석기 AI 사용
├── 평상시: 핵심 분석기만 AI 사용
└── 데이터 부족 시: rule_based 분석 사용
```

## 🛠 구현 순서

### Phase 1: 긴급 수정 (즉시)
1. `data_scheduler.py`에 AI 분석 결과 저장 기능 추가
2. `run_all_analyses()` 함수를 결과 조회 함수로 변경
3. AI API 호출량 250회 이하로 감소 확인

### Phase 2: 안정성 강화 (1-2일)
1. API 실패 시 AI 분석 스킵 로직 구현
2. 에러 카운트 3회 초과 시 처리 방식 구현
3. 최종 결정에 분석 시간 메타데이터 추가

### Phase 3: 성능 최적화 (1주)
1. 스마트 스케줄링 시스템 구현
2. 조건부 AI 호출 로직 구현
3. 데이터 품질 기반 결정 신뢰도 계산

### Phase 4: 모니터링 (추가)
1. API 사용량 실시간 모니터링
2. 분석 품질 지표 대시보드
3. 에러 패턴 분석 및 알림

## 📝 주요 수정 파일 예상

```
🔧 긴급 수정:
├── docs/investment_ai/data_scheduler.py (AI 분석 결과 저장)
├── docs/investment_ai/ai_trading_integration.py (결과 조회로 변경)
└── docs/investment_ai/final_decisionmaker.py (메타데이터 추가)

🔧 추가 수정:
├── docs/investment_ai/analyzers/*.py (조건부 AI 호출)
├── main_ai.py (스케줄링 로직 조정)
└── 모니터링 대시보드 (신규)
```

## 🎯 성공 지표

- **AI API 사용량**: 672회/일 → 200회/일 이하
- **시스템 안정성**: API 실패 시에도 거래 지속
- **결정 품질**: 메타데이터 기반 신뢰도 표시
- **운영 효율성**: 중복 작업 제거로 응답 속도 향상

## ⚠ 위험 요소

- **기존 거래 로직 변경**: 충분한 테스트 필요
- **데이터 정합성**: 캐시된 분석 결과의 유효성 검증 필요  
- **에러 처리**: 예상치 못한 실패 케이스 대비
- **성능 저하**: 분석 결과 저장으로 인한 지연 가능성