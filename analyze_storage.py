#!/usr/bin/env python3
"""
MongoDB ì €ì¥ì†Œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- í˜„ì¬ ë°ì´í„° ì‚¬ìš©ëŸ‰ ì¡°ì‚¬
- ê° ì»¬ë ‰ì…˜ë³„ ì¦ê°€ìœ¨ ë¶„ì„
- TTL ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
- ì €ì¥ì†Œ ìµœì í™” ì œì•ˆ
"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from pymongo import MongoClient
from typing import Dict, List, Any
import os
import sys

# ë¡œê¹… ì„¤ì •
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StorageAnalyzer:
    def __init__(self):
        self.client = MongoClient("mongodb://mongodb:27017")
        self.db = self.client["bitcoin"]
        
    def get_database_stats(self) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„"""
        try:
            stats = self.db.command("dbStats")
            return {
                "database_size_mb": round(stats.get("dataSize", 0) / (1024 * 1024), 2),
                "storage_size_mb": round(stats.get("storageSize", 0) / (1024 * 1024), 2),
                "index_size_mb": round(stats.get("indexSize", 0) / (1024 * 1024), 2),
                "total_collections": stats.get("collections", 0),
                "total_objects": stats.get("objects", 0),
                "avg_object_size": round(stats.get("avgObjSize", 0), 2)
            }
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_collection_stats(self) -> List[Dict]:
        """ê° ì»¬ë ‰ì…˜ë³„ ìƒì„¸ í†µê³„"""
        collections_info = []
        
        try:
            collection_names = self.db.list_collection_names()
            
            for collection_name in collection_names:
                try:
                    collection = self.db[collection_name]
                    stats = self.db.command("collStats", collection_name)
                    
                    # ë¬¸ì„œ ìˆ˜
                    count = collection.count_documents({})
                    
                    # ìµœê·¼ ë¬¸ì„œ ì‹œê°„ (created_at ë˜ëŠ” timestamp í•„ë“œê°€ ìˆëŠ” ê²½ìš°)
                    latest_doc = None
                    oldest_doc = None
                    
                    try:
                        # created_at í•„ë“œë¡œ ì‹œë„
                        latest_doc = collection.find_one({}, sort=[("created_at", -1)])
                        oldest_doc = collection.find_one({}, sort=[("created_at", 1)])
                        
                        if not latest_doc:
                            # timestamp í•„ë“œë¡œ ì‹œë„
                            latest_doc = collection.find_one({}, sort=[("timestamp", -1)])
                            oldest_doc = collection.find_one({}, sort=[("timestamp", 1)])
                    except:
                        pass
                    
                    # ì¸ë±ìŠ¤ ì •ë³´
                    indexes = list(collection.list_indexes())
                    ttl_indexes = []
                    for idx in indexes:
                        if 'expireAfterSeconds' in idx:
                            ttl_indexes.append({
                                'name': idx['name'],
                                'field': list(idx['key'].keys())[0],
                                'ttl_seconds': idx['expireAfterSeconds']
                            })
                    
                    collection_info = {
                        "name": collection_name,
                        "document_count": count,
                        "size_mb": round(stats.get("size", 0) / (1024 * 1024), 2),
                        "storage_size_mb": round(stats.get("storageSize", 0) / (1024 * 1024), 2),
                        "avg_object_size": round(stats.get("avgObjSize", 0), 2),
                        "total_indexes": stats.get("nindexes", 0),
                        "index_size_mb": round(stats.get("totalIndexSize", 0) / (1024 * 1024), 2),
                        "ttl_indexes": ttl_indexes,
                        "latest_document": latest_doc.get('created_at') or latest_doc.get('timestamp') if latest_doc else None,
                        "oldest_document": oldest_doc.get('created_at') or oldest_doc.get('timestamp') if oldest_doc else None
                    }
                    
                    collections_info.append(collection_info)
                    
                except Exception as e:
                    logger.warning(f"ì»¬ë ‰ì…˜ {collection_name} í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    collections_info.append({
                        "name": collection_name,
                        "error": str(e)
                    })
            
            # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
            collections_info.sort(key=lambda x: x.get("size_mb", 0), reverse=True)
            return collections_info
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_growth_pattern(self) -> Dict:
        """ë°ì´í„° ì¦ê°€ íŒ¨í„´ ë¶„ì„"""
        growth_analysis = {}
        
        try:
            # ì£¼ìš” ë°ì´í„° ì»¬ë ‰ì…˜ë“¤ì˜ ì¦ê°€ íŒ¨í„´ ë¶„ì„
            target_collections = [
                "data_cache",       # AI ë¶„ì„ ê²°ê³¼ ìºì‹œ
                "chart_15m",        # 15ë¶„ ì°¨íŠ¸ ë°ì´í„°
                "chart_5m",         # 5ë¶„ ì°¨íŠ¸ ë°ì´í„° (ì‚¬ìš© ì•ˆí•¨)
                "trades",           # ê±°ë˜ ê¸°ë¡
                "positions",        # í¬ì§€ì…˜ ì •ë³´
                "logs"              # ë¡œê·¸ ë°ì´í„°
            ]
            
            for collection_name in target_collections:
                if collection_name in self.db.list_collection_names():
                    collection = self.db[collection_name]
                    
                    # ìµœê·¼ 7ì¼ê°„ ë¬¸ì„œ ìˆ˜ ë¶„ì„
                    now = datetime.now(timezone.utc)
                    daily_counts = []
                    
                    for i in range(7):
                        day_start = now - timedelta(days=i+1)
                        day_end = now - timedelta(days=i)
                        
                        try:
                            day_count = collection.count_documents({
                                "created_at": {
                                    "$gte": day_start,
                                    "$lt": day_end
                                }
                            })
                        except:
                            # created_atì´ ì—†ìœ¼ë©´ timestampë¡œ ì‹œë„
                            try:
                                day_count = collection.count_documents({
                                    "timestamp": {
                                        "$gte": day_start.timestamp(),
                                        "$lt": day_end.timestamp()
                                    }
                                })
                            except:
                                day_count = 0
                        
                        daily_counts.append({
                            "date": day_start.strftime("%Y-%m-%d"),
                            "count": day_count
                        })
                    
                    # ì¼í‰ê·  ì¦ê°€ëŸ‰ ê³„ì‚°
                    total_docs = sum(day["count"] for day in daily_counts)
                    avg_daily_growth = total_docs / 7 if total_docs > 0 else 0
                    
                    growth_analysis[collection_name] = {
                        "daily_counts": daily_counts,
                        "avg_daily_growth": round(avg_daily_growth, 1),
                        "estimated_monthly_growth": round(avg_daily_growth * 30, 1),
                        "total_last_week": total_docs
                    }
        
        except Exception as e:
            logger.error(f"ì¦ê°€ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return growth_analysis
    
    def check_ttl_effectiveness(self) -> Dict:
        """TTL ì¸ë±ìŠ¤ íš¨ê³¼ ë¶„ì„"""
        ttl_analysis = {}
        
        try:
            collection_names = self.db.list_collection_names()
            
            for collection_name in collection_names:
                collection = self.db[collection_name]
                indexes = list(collection.list_indexes())
                
                for idx in indexes:
                    if 'expireAfterSeconds' in idx:
                        field_name = list(idx['key'].keys())[0]
                        ttl_seconds = idx['expireAfterSeconds']
                        
                        # TTL ë§Œë£Œ ì˜ˆì • ë¬¸ì„œ ìˆ˜ í™•ì¸
                        cutoff_time = datetime.now(timezone.utc) - timedelta(seconds=ttl_seconds)
                        
                        try:
                            expired_count = collection.count_documents({
                                field_name: {"$lt": cutoff_time}
                            })
                        except:
                            expired_count = 0
                        
                        total_count = collection.count_documents({})
                        
                        ttl_analysis[f"{collection_name}.{field_name}"] = {
                            "collection": collection_name,
                            "field": field_name,
                            "ttl_hours": round(ttl_seconds / 3600, 1),
                            "total_documents": total_count,
                            "expired_documents": expired_count,
                            "retention_rate": round((total_count - expired_count) / max(1, total_count) * 100, 1)
                        }
        
        except Exception as e:
            logger.error(f"TTL íš¨ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return ttl_analysis
    
    def get_storage_recommendations(self, db_stats: Dict, collections: List[Dict], growth: Dict) -> List[str]:
        """ì €ì¥ì†Œ ìµœì í™” ì œì•ˆ"""
        recommendations = []
        
        # 1. ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ê¸°ì¤€
        total_size_mb = db_stats.get("database_size_mb", 0)
        if total_size_mb > 1000:  # 1GB ì´ìƒ
            recommendations.append(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°ê°€ {total_size_mb}MBë¡œ í¼ - ì •ë¦¬ í•„ìš”")
        elif total_size_mb > 500:  # 500MB ì´ìƒ
            recommendations.append(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° {total_size_mb}MB - ëª¨ë‹ˆí„°ë§ í•„ìš”")
        
        # 2. í° ì»¬ë ‰ì…˜ í™•ì¸
        for col in collections[:3]:  # ìƒìœ„ 3ê°œ ì»¬ë ‰ì…˜
            if col.get("size_mb", 0) > 100:
                recommendations.append(f"ğŸ“Š ëŒ€ìš©ëŸ‰ ì»¬ë ‰ì…˜ '{col['name']}': {col['size_mb']}MB - ìµœì í™” ê²€í† ")
        
        # 3. TTL ì¸ë±ìŠ¤ ëˆ„ë½ í™•ì¸
        cache_collections = ["data_cache", "chart_15m", "chart_5m"]
        for col in collections:
            if col["name"] in cache_collections and not col.get("ttl_indexes"):
                recommendations.append(f"â° '{col['name']}' ì»¬ë ‰ì…˜ì— TTL ì¸ë±ìŠ¤ ì¶”ê°€ ê¶Œì¥")
        
        # 4. ê¸‰ì†í•œ ì¦ê°€ íŒ¨í„´ ê°ì§€
        for col_name, growth_data in growth.items():
            monthly_growth = growth_data.get("estimated_monthly_growth", 0)
            if monthly_growth > 10000:  # ì›” 1ë§Œê°œ ì´ìƒ ì¦ê°€
                recommendations.append(f"ğŸ“ˆ '{col_name}' ì»¬ë ‰ì…˜ ê¸‰ì¦ - ì›” {monthly_growth}ê°œ ë¬¸ì„œ ì˜ˆìƒ")
        
        # 5. ì¸ë±ìŠ¤ í¬ê¸° í™•ì¸
        for col in collections:
            if col.get("index_size_mb", 0) > col.get("size_mb", 0) * 0.5:
                recommendations.append(f"ğŸ” '{col['name']}' ì¸ë±ìŠ¤ í¬ê¸° ê³¼ë‹¤ - ìµœì í™” í•„ìš”")
        
        return recommendations
    
    def generate_report(self) -> Dict:
        """ì¢…í•© ì €ì¥ì†Œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("MongoDB ì €ì¥ì†Œ ë¶„ì„ ì‹œì‘...")
        
        # ë°ì´í„° ìˆ˜ì§‘
        db_stats = self.get_database_stats()
        collections = self.get_collection_stats()
        growth_patterns = self.analyze_growth_pattern()
        ttl_analysis = self.check_ttl_effectiveness()
        recommendations = self.get_storage_recommendations(db_stats, collections, growth_patterns)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
            "database_overview": db_stats,
            "collections_analysis": collections,
            "growth_patterns": growth_patterns,
            "ttl_effectiveness": ttl_analysis,
            "recommendations": recommendations,
            "summary": {
                "total_collections": len(collections),
                "largest_collection": collections[0]["name"] if collections else None,
                "largest_collection_size_mb": collections[0].get("size_mb", 0) if collections else 0,
                "collections_with_ttl": len([c for c in collections if c.get("ttl_indexes")]),
                "high_growth_collections": len([k for k, v in growth_patterns.items() 
                                               if v.get("estimated_monthly_growth", 0) > 5000])
            }
        }
        
        logger.info("MongoDB ì €ì¥ì†Œ ë¶„ì„ ì™„ë£Œ")
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        analyzer = StorageAnalyzer()
        report = analyzer.generate_report()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š MONGODB ì €ì¥ì†Œ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("="*80)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”
        db_stats = report["database_overview"]
        print(f"\nğŸ—„ï¸  ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”:")
        print(f"   â€¢ ì „ì²´ í¬ê¸°: {db_stats.get('database_size_mb', 0)}MB")
        print(f"   â€¢ ì €ì¥ ê³µê°„: {db_stats.get('storage_size_mb', 0)}MB")
        print(f"   â€¢ ì¸ë±ìŠ¤ í¬ê¸°: {db_stats.get('index_size_mb', 0)}MB")
        print(f"   â€¢ ì´ ë¬¸ì„œ ìˆ˜: {db_stats.get('total_objects', 0):,}ê°œ")
        
        # ì£¼ìš” ì»¬ë ‰ì…˜
        collections = report["collections_analysis"]
        print(f"\nğŸ“‹ ì£¼ìš” ì»¬ë ‰ì…˜ (ìƒìœ„ 5ê°œ):")
        for i, col in enumerate(collections[:5]):
            if "error" not in col:
                ttl_info = f" (TTL: {len(col.get('ttl_indexes', []))}ê°œ)" if col.get('ttl_indexes') else ""
                print(f"   {i+1}. {col['name']}: {col['size_mb']}MB, {col['document_count']:,}ê°œ ë¬¸ì„œ{ttl_info}")
        
        # ì¦ê°€ íŒ¨í„´
        growth = report["growth_patterns"]
        print(f"\nğŸ“ˆ ë°ì´í„° ì¦ê°€ íŒ¨í„´ (ìµœê·¼ 7ì¼):")
        for col_name, growth_data in growth.items():
            if growth_data.get("avg_daily_growth", 0) > 0:
                print(f"   â€¢ {col_name}: ì¼í‰ê·  {growth_data['avg_daily_growth']}ê°œ, ì›”ì˜ˆìƒ {growth_data['estimated_monthly_growth']}ê°œ")
        
        # TTL ì¸ë±ìŠ¤ ìƒíƒœ
        ttl_data = report["ttl_effectiveness"]
        if ttl_data:
            print(f"\nâ° TTL ì¸ë±ìŠ¤ í˜„í™©:")
            for ttl_name, ttl_info in ttl_data.items():
                print(f"   â€¢ {ttl_info['collection']}.{ttl_info['field']}: {ttl_info['ttl_hours']}ì‹œê°„, ë³´ì¡´ìœ¨ {ttl_info['retention_rate']}%")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = report["recommendations"]
        if recommendations:
            print(f"\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations):
                print(f"   {i+1}. {rec}")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"storage_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*80)
        
        return report
        
    except Exception as e:
        logger.error(f"ì €ì¥ì†Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

if __name__ == "__main__":
    main()